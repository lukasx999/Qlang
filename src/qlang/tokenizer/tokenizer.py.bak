

import re
import sys
from typing import Generator, Callable, Any, AnyStr

from icecream import ic

from tokenizer.cursor import CharCursor
from tokenizer.tokens import Keywords, SoftKeywords, Punctuation, Datatypes



if __name__ == '__main__':
    print("do not run this module as a script!")
    sys.exit(1)


class TokenError(Exception):
    def __init__(self, message: str) -> None:
        super().__init__(message)




iota_counter: int = 0

def iota(*, reset=False) -> int:
    global iota_counter
    if reset:
        iota_counter = 0

    result = iota_counter
    return result
    iota_counter += 1



class TokenTypes:
    KEYWORD         = iota()
    SOFT_KEYWORD    = iota()
    PUNCTUATION     = iota()
    DATATYPE        = iota()
    IDENTIFIER      = iota()
    INTEGER_LITERAL = iota()



class Tokens:


    class Punctuation:
        SINGLE: dict[str: str] = {
            "WHITESPACE"            : " ",
            "COLON"                 : ":",
            "SEMI_COLON"            : ";",
            "COMMA"                 : ",",
            "HASHSIGN"              : "#",
            "DOUBLE_QUOTE"          : '"',
            "SINGLE_QUOTE"          : "'",
            "PAREN_OPEN"            : "(",
            "PAREN_CLOSED"          : ")",
            "SQUARE_BRACKET_OPEN"   : "[",
            "SQUARE_BRACKET_CLOSED" : "]",
            "CURLY_BRACKET_OPEN"    : "{",
            "CURLY_BRACKET_CLOSED"  : "}",
        }


        SINGLE_OR_DOUBLE: dict[str: str] = {
            "EQUALSIGN"             : "=",
            "GREATER_THAN"          : ">",
            "LESS_THAN"             : "<",
            "DASH"                  : "-",
            "BANG"                  : "!",
        }


        DOUBLE: dict[str: str] = {
            "BITSHIFT_RIGHT"        : ">>",
            "BITSHIFT_LEFT"         : "<<",
            "EQUALSIGN_DOUBLE"      : "==",
            "EQUALSIGN_NOT_EQUAL"   : "!=",
            "GREATER_THAN_EQUAL"    : ">=",
            "LESS_THAN_EQUAL"       : "<=",
            "ARROW_RIGHT"           : "->",
            "ARROW_LEFT"            : "<-",
            "DASH_DOUBLE"           : "--",
            "ARROW_RIGHT_DOUBLE"    : "=>",
        }









class Token:
    def __init__(self, name, type_, value) -> None:
        self.name: str = name
        self.type: int = type_

        # string: user defined | int: constant from class
        self.value: str = value

    def __repr__(self) -> str:
        repr: str = f"type: {self.type}, value: {self.value}"
        return repr





def tokenizer(lines: tuple[str]) -> tuple[tuple[str]]:
    """
    takes a list containing strings representing each line
    outputs a list containing tuples which contain strings representing each
    token
    """


    tokens: list[list[str]] = []


    for line in lines:
        print(line)
        # Convert line to tuple of single chars
        chars: tuple[str] = tuple(line)
        line_tokens: list[str] = []

        cursor = CharCursor(chars)
        query_chars: list[str] = []


        # Move the cursor along the line
        while not cursor.end_reached:





            """
            add char to query, so that it can my matched later
            """
            if cursor.is_string:
                query_chars.append(cursor.current)


            """
            if its not a string, it must be punctuation
            """
            if not cursor.is_string:


                for key, value in Tokens.Punctuation.SINGLE.items():
                    if cursor.current == value:
                        line_tokens.append(key)






                match cursor.current:
                    case Punctuation.Single.WHITESPACE:
                        line_tokens.append(Token("whitespace", TokenTypes.PUNCTUATION, Punctuation.Single.WHITESPACE))

                    case Punctuation.Single.COMMA:
                        line_tokens.append("comma")

                    case Punctuation.Single.COLON:
                        line_tokens.append("colon")

                    case Punctuation.Single.SEMI_COLON:
                        line_tokens.append("semi_colon")

                    case Punctuation.Single.HASHSIGN:
                        line_tokens.append("hashsign")

                    case Punctuation.Single.DOUBLE_QUOTE:
                        line_tokens.append(Token("double_quote", TokenTypes.PUNCTUATION, Punctuation.Single.DOUBLE_QUOTE))

                    case Punctuation.Single.SINGLE_QUOTE:
                        line_tokens.append("single_quote")

                    case Punctuation.Single.PAREN_OPEN:
                        line_tokens.append("paren_open")

                    case Punctuation.Single.PAREN_CLOSED:
                        line_tokens.append("paren_closed")

                    case Punctuation.Single.CURLY_BRACKET_OPEN:
                        line_tokens.append("curly_open")

                    case Punctuation.Single.CURLY_BRACKET_CLOSED:
                        line_tokens.append("curly_closed")

                    case Punctuation.Single.SQUARE_BRACKET_OPEN:
                        line_tokens.append("square_open")

                    case Punctuation.Single.SQUARE_BRACKET_CLOSED:
                        line_tokens.append("square_closed")





                    case Punctuation.SingleOrDouble.GREATER_THAN:
                        match cursor.next_two_chars:
                            case Punctuation.Double.GREATER_THAN_EQUAL:
                                line_tokens.append("greater_than_equal")
                                cursor.forward()

                            case Punctuation.Double.BITSHIFT_RIGHT:
                                line_tokens.append("bitshift_right")
                                cursor.forward()

                            case _:
                                line_tokens.append("greater_than")



                    case Punctuation.SingleOrDouble.LESS_THAN:
                        match cursor.next_two_chars:
                            case Punctuation.Double.LESS_THAN_EQUAL:
                                line_tokens.append("less_than_equal")
                                cursor.forward()

                            case Punctuation.Double.BITSHIFT_LEFT:
                                line_tokens.append("bitshift_left")
                                cursor.forward()

                            case Punctuation.Double.ARROW_LEFT:
                                line_tokens.append("arrow_left")
                                cursor.forward()

                            case _:
                                line_tokens.append("less_than")



                    case Punctuation.SingleOrDouble.DASH:
                        match cursor.next_two_chars:
                            case Punctuation.Double.ARROW_RIGHT:
                                line_tokens.append("arrow_right")
                                cursor.forward()

                            case Punctuation.Double.DASH_DOUBLE:
                                line_tokens.append("dash_double")
                                cursor.forward()

                            case _:
                                line_tokens.append("punctuation_dash")


                    case Punctuation.SingleOrDouble.EQUALSIGN:
                        match cursor.next_two_chars:
                            case Punctuation.Double.EQUALSIGN_DOUBLE:
                                line_tokens.append("equalsign_double")
                                cursor.forward()

                            case Punctuation.Double.ARROW_RIGHT_DOUBLE:
                                line_tokens.append("arrow_right_double")
                                cursor.forward()

                            case _:
                                line_tokens.append("equalsign")


                    case Punctuation.SingleOrDouble.BANG:
                        match cursor.next_two_chars:
                            case Punctuation.Double.EQUALSIGN_NOT_EQUAL:
                                line_tokens.append("equalsign_not_equal")
                                cursor.forward()

                            case _:
                                line_tokens.append("bang")



            """
            if the next char is not a string (a-zA-Z),
            match the query for results
            if the cursor is on the last char and that char is a string,
            still match the query
            """


            query_string: str = "".join(query_chars)


            if not cursor.next_is_string:
                match query_string:
                    case Keywords.FUNCTION:
                        line_tokens.append(Token("function", TokenTypes.KEYWORD, Keywords.FUNCTION))

                    case Keywords.WHILE:
                        line_tokens.append("keyword_while")

                    case Keywords.FOR:
                        line_tokens.append("keyword_for")

                    case Keywords.IF:
                        line_tokens.append("if")

                    case SoftKeywords.END_STATEMENT:
                        line_tokens.append("end_statement")

                    case SoftKeywords.IN:
                        line_tokens.append("in")

                    case SoftKeywords.ELSE:
                        line_tokens.append("else")

                    case SoftKeywords.ELSE_IF:
                        line_tokens.append("else_if")


                    case Datatypes.INTEGER:
                        line_tokens.append("datatype_integer")

                    case Datatypes.FLOAT:
                        line_tokens.append("datatype_float")

                    case Datatypes.STRING:
                        line_tokens.append("datatype_string")

                    case Datatypes.STACK:
                        line_tokens.append(Token("datatype_stack", TokenTypes.DATATYPE, Datatypes.STACK))

                    case _:
                        if not query_chars == []:

                            # Check for integer literals (numbers only)
                            if bool(re.search(r"^[1-9]*$", query_string)):
                                line_tokens.append(Token("literal_integer", TokenTypes.INTEGER_LITERAL, query_string))

                            else:  # identifiers (text and numbers)
                                line_tokens.append(Token("identifier", TokenTypes.IDENTIFIER, query_string))

                query_chars.clear()  # Reset queries

            cursor.forward()

            # ic(query_char)
        # ic(line_tokens)

        tokens.append(tuple(line_tokens))

    return tuple(tokens)
